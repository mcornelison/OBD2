# B-016: Remote Ollama Server Integration

| Field        | Value         |
|--------------|---------------|
| Priority     | Medium        |
| Status       | Groomed       |
| Category     | infrastructure|
| Size         | M             |
| Related PRD  | pm/prds/prd-remote-ollama.md |
| Dependencies | None          |
| Created      | 2026-01-29    |

## Description

Instead of running Ollama directly on the Pi 5 (which would consume most of the 8GB RAM and be slow), configure the application to connect to a remote Ollama server running on a more powerful machine on the local network.

This requires:

1. **Investigation**: Determine the best host machine (desktop, NAS, server) and network setup
2. **Configuration**: Add remote Ollama host/port to config.json
3. **Code changes**: Ensure the AI module connects to a remote URL instead of localhost
4. **Fallback**: Graceful degradation when the remote server is unreachable (already partially implemented, see B-003)
5. **Documentation**: Setup guide for the Ollama server and network configuration

### Architecture

```
┌──────────────┐         WiFi/LAN         ┌──────────────────┐
│   Pi 5       │ ──── HTTP :11434 ────→   │  Ollama Server   │
│  (in car)    │                          │  (home network)  │
│              │ ←── JSON response ────   │  Gemma2 / Qwen   │
└──────────────┘                          └──────────────────┘
```

**Note**: AI analysis only runs post-drive (when vehicle is parked and likely on home WiFi), so network availability during driving is not a concern.

## Acceptance Criteria

- [ ] Config supports remote Ollama URL (e.g., `http://192.168.1.100:11434`)
- [ ] AI module connects to configured remote URL instead of localhost
- [ ] Health check verifies remote Ollama is reachable before attempting analysis
- [ ] Graceful fallback when remote server is unreachable (no crash, log warning)
- [ ] Timeout handling for slow network responses
- [ ] Model availability check on remote server (verify required model is loaded)
- [ ] Documentation: how to set up Ollama server, firewall rules, model installation

## Validation Script Requirements

- **Input**: Configure remote Ollama URL in config.json, trigger post-drive analysis
- **Expected Output**: AI recommendations generated from remote model, logged and stored
- **Database State**: `ai_recommendations` table contains entries with remote-generated content
- **Test Program**: Write tests that mock the remote Ollama endpoint (different URL than localhost) and verify the full analysis flow works. Also test timeout and unreachable scenarios.

## Notes

Ollama's API is the same whether local or remote -- just a different base URL. The main work is making the URL configurable (currently may be hardcoded to localhost) and ensuring the post-drive timing aligns with WiFi availability. Consider adding a network connectivity check before attempting remote AI calls.
