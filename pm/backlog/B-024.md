# B-024: Remove Local Ollama References from Codebase

| Field        | Value                  |
|--------------|------------------------|
| Priority     | High                   |
| Status       | Pending                |
| Category     | cleanup                |
| Size         | S                      |
| Related PRD  | None                   |
| Dependencies | B-016                  |
| Created      | 2026-01-31             |

## Description

The codebase currently contains references to running Ollama locally on the Pi 5 (localhost:11434). Per CIO decision, Ollama will **never** run on the Pi -- all LLM work is handled by Chi-srv-01 on the local network. Clean up any code, config, comments, or documentation that suggests or supports running Ollama locally on the Pi.

### What to Clean Up

1. **Config defaults**: `ollamaBaseUrl: "http://localhost:11434"` should not suggest localhost is the default for Pi production
2. **Architecture docs**: Any reference to "local LLM inference" or "local HTTP (port 11434)" on the Pi
3. **Comments/docstrings**: Any mention of running Ollama on the Pi or local Ollama on the target device
4. **Hardware specs**: Architecture doc says "8GB RAM for AI inference" for Pi 5 -- this is no longer the reason for 8GB
5. **Agent knowledge**: Ollama tips in `ralph/agent.md` should clarify remote-only pattern

### What NOT to Clean Up

- The `ollamaBaseUrl` config key itself -- it's still needed, just pointed at Chi-srv-01
- The OllamaManager code -- it works fine with remote URLs
- Health checks and graceful fallback -- still needed for remote server
- Tests that mock Ollama -- they test the integration layer regardless of locality

## Acceptance Criteria

- [ ] No code or config suggests Ollama runs locally on the Pi 5
- [ ] Config default for production environment points to Chi-srv-01 (or uses `${OLLAMA_BASE_URL}` env var)
- [ ] Architecture docs updated: Pi connects to Chi-srv-01 for AI, not local
- [ ] `specs/architecture.md` External Dependencies table: ollama listed as "Remote HTTP (Chi-srv-01, port 11434)" not "Local HTTP"
- [ ] Pi 5 hardware description: 8GB RAM is for application headroom, not AI inference
- [ ] All tests pass after cleanup
- [ ] Typecheck passes

## Notes

- This is a codebase hygiene task. No functional changes to the OllamaManager -- it already supports remote URLs.
- Should be done after B-016 (Remote Ollama PRD) since B-016 makes the URL properly configurable.
- Keep it focused -- don't refactor the AI module, just clean up misleading references.
